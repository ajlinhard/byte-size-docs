# Using Avro and ORC in Pyspark and Pandas
Yes, both pandas and PySpark can read Avro and ORC files, though with different levels of native support and some caveats. Here's a comprehensive overview:## **Quick Summary:**

### **Pandas Support:**

**Avro Files:**
- ‚úÖ **Supported** via third-party libraries (`fastavro`, `pandavro`)
- üèÜ **Best option**: `fastavro` for performance
- ‚ùå **No native support** in pandas

**ORC Files:**
- ‚úÖ **Native support** since pandas 1.5.0 with `pd.read_orc()`
- ‚úÖ **PyArrow backend** provides excellent performance
- üèÜ **Best option**: `pd.read_orc()` or PyArrow directly

### **PySpark Support:**

**Avro Files:**
- ‚úÖ **Excellent support** with spark-avro package
- üèÜ **Best for**: Large distributed datasets
- üì¶ **Requires**: `spark-avro` package configuration

**ORC Files:**
- ‚úÖ **Native built-in support** (no extra packages needed)
- üèÜ **Exceptional performance** for analytics workloads
- ‚≠ê **Highly optimized** for big data scenarios

## **Key Installation Commands:**

```bash
# For pandas
pip install pandas pyarrow fastavro

# For PySpark  
pip install pyspark

# Spark configuration for Avro
# Add to SparkSession: .config("spark.jars.packages", "org.apache.spark:spark-avro_2.12:3.4.0")
```

## **When to Use What:**

- **Small files + Quick analysis**: Pandas with appropriate libraries
- **Large files + Distributed processing**: PySpark
- **Analytics workloads**: PySpark + ORC (best performance)
- **Schema evolution needs**: Avro format with either tool
- **Data warehousing**: PySpark + ORC
- **Data exchange/streaming**: Avro format

Both tools handle these formats well, but PySpark excels with larger datasets while pandas is perfect for smaller-scale analysis and exploration.

---
# Code:
```python
# ============================================================================
# READING AVRO AND ORC FILES IN PANDAS AND PYSPARK
# ============================================================================

import pandas as pd
from pyspark.sql import SparkSession
import os

# Initialize Spark Session (needed for PySpark examples)
def create_spark_session():
    """Create Spark session with necessary configurations"""
    spark = SparkSession.builder \
        .appName("AvroOrcExample") \
        .config("spark.jars.packages", 
                "org.apache.spark:spark-avro_2.12:3.4.0,org.apache.orc:orc-core:1.8.0") \
        .getOrCreate()
    return spark

# ============================================================================
# PANDAS - AVRO FILES
# ============================================================================

def read_avro_with_pandas_fastavro(file_path):
    """
    Read Avro files in pandas using fastavro
    Installation: pip install fastavro
    """
    try:
        import fastavro
        import io
        
        records = []
        with open(file_path, 'rb') as f:
            reader = fastavro.reader(f)
            for record in reader:
                records.append(record)
        
        df = pd.DataFrame(records)
        print(f"‚úÖ Pandas + fastavro: Read {len(df)} rows from {file_path}")
        return df
    
    except ImportError:
        print("‚ùå fastavro not installed. Run: pip install fastavro")
        return None

def read_avro_with_pandas_pandavro(file_path):
    """
    Read Avro files in pandas using pandavro
    Installation: pip install pandavro
    """
    try:
        import pandavro as pdx
        
        df = pdx.read_avro(file_path)
        print(f"‚úÖ Pandas + pandavro: Read {len(df)} rows from {file_path}")
        return df
    
    except ImportError:
        print("‚ùå pandavro not installed. Run: pip install pandavro")
        return None

# ============================================================================
# PANDAS - ORC FILES  
# ============================================================================

def read_orc_with_pandas_pyarrow(file_path):
    """
    Read ORC files in pandas using PyArrow
    Installation: pip install pyarrow
    """
    try:
        import pyarrow.orc as orc
        
        # Read ORC file
        table = orc.read_table(file_path)
        df = table.to_pandas()
        
        print(f"‚úÖ Pandas + PyArrow: Read {len(df)} rows from {file_path}")
        return df
    
    except ImportError:
        print("‚ùå PyArrow not installed. Run: pip install pyarrow")
        return None
    except Exception as e:
        print(f"‚ùå Error reading ORC file: {e}")
        return None

def read_orc_with_pandas_native(file_path):
    """
    Read ORC files using pandas native support (pandas >= 1.5.0)
    """
    try:
        # Pandas native ORC reading (requires pyarrow backend)
        df = pd.read_orc(file_path)
        print(f"‚úÖ Pandas native: Read {len(df)} rows from {file_path}")
        return df
    
    except Exception as e:
        print(f"‚ùå Error with pandas native ORC reading: {e}")
        print("üí° Make sure you have pandas >= 1.5.0 and pyarrow installed")
        return None

# ============================================================================
# PYSPARK - AVRO FILES
# ============================================================================

def read_avro_with_pyspark(spark, file_path):
    """
    Read Avro files with PySpark
    Requires spark-avro package
    """
    try:
        # Read Avro file
        df = spark.read.format("avro").load(file_path)
        
        print(f"‚úÖ PySpark Avro: Read {df.count()} rows from {file_path}")
        print("Schema:")
        df.printSchema()
        
        return df
    
    except Exception as e:
        print(f"‚ùå Error reading Avro with PySpark: {e}")
        print("üí° Make sure spark-avro package is available")
        return None

def write_avro_with_pyspark(spark, df, output_path):
    """
    Write DataFrame as Avro file with PySpark
    """
    try:
        df.write.format("avro").mode("overwrite").save(output_path)
        print(f"‚úÖ PySpark: Written Avro file to {output_path}")
    
    except Exception as e:
        print(f"‚ùå Error writing Avro with PySpark: {e}")

# ============================================================================
# PYSPARK - ORC FILES
# ============================================================================

def read_orc_with_pyspark(spark, file_path):
    """
    Read ORC files with PySpark (native support)
    """
    try:
        # PySpark has native ORC support
        df = spark.read.format("orc").load(file_path)
        
        print(f"‚úÖ PySpark ORC: Read {df.count()} rows from {file_path}")
        print("Schema:")
        df.printSchema()
        
        return df
    
    except Exception as e:
        print(f"‚ùå Error reading ORC with PySpark: {e}")
        return None

def write_orc_with_pyspark(spark, df, output_path):
    """
    Write DataFrame as ORC file with PySpark
    """
    try:
        df.write.format("orc").mode("overwrite").save(output_path)
        print(f"‚úÖ PySpark: Written ORC file to {output_path}")
    
    except Exception as e:
        print(f"‚ùå Error writing ORC with PySpark: {e}")

# ============================================================================
# COMPARISON AND EXAMPLES
# ============================================================================

def create_sample_data():
    """Create sample data for testing"""
    data = {
        'id': [1, 2, 3, 4, 5],
        'name': ['Alice', 'Bob', 'Charlie', 'Diana', 'Eve'],
        'age': [25, 30, 35, 28, 32],
        'salary': [50000, 60000, 70000, 55000, 65000],
        'department': ['Engineering', 'Marketing', 'Sales', 'HR', 'Engineering']
    }
    return pd.DataFrame(data)

def comprehensive_example():
    """
    Comprehensive example showing all methods
    """
    print("üöÄ COMPREHENSIVE AVRO & ORC READING EXAMPLE")
    print("=" * 60)
    
    # Create sample data
    sample_df = create_sample_data()
    print("üìä Sample data created:")
    print(sample_df)
    print()
    
    # Initialize Spark
    spark = create_spark_session()
    spark_df = spark.createDataFrame(sample_df)
    
    # Test file paths
    avro_file = "test_data.avro"
    orc_file = "test_data.orc"
    
    print("üìù WRITING TEST FILES")
    print("-" * 30)
    
    # Write test files using PySpark
    write_avro_with_pyspark(spark, spark_df, avro_file)
    write_orc_with_pyspark(spark, spark_df, orc_file)
    print()
    
    print("üìñ READING AVRO FILES")
    print("-" * 30)
    
    # Read Avro with different methods
    df_avro_fastavro = read_avro_with_pandas_fastavro(avro_file + "/part-00000-*.avro")
    df_avro_pandavro = read_avro_with_pandas_pandavro(avro_file + "/part-00000-*.avro")
    df_avro_pyspark = read_avro_with_pyspark(spark, avro_file)
    print()
    
    print("üìñ READING ORC FILES")
    print("-" * 30)
    
    # Read ORC with different methods
    df_orc_pyarrow = read_orc_with_pandas_pyarrow(orc_file + "/part-00000-*.orc")
    df_orc_pandas = read_orc_with_pandas_native(orc_file + "/part-00000-*.orc")
    df_orc_pyspark = read_orc_with_pyspark(spark, orc_file)
    print()
    
    # Clean up
    spark.stop()
    
    print("‚ú® Example completed!")

# ============================================================================
# PERFORMANCE COMPARISON
# ============================================================================

def performance_comparison_tips():
    """
    Performance tips for different scenarios
    """
    tips = """
    üöÄ PERFORMANCE COMPARISON & TIPS
    ================================
    
    üìä PANDAS PERFORMANCE:
    ----------------------
    Avro:
    ‚Ä¢ fastavro: ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê (Fastest for pandas)
    ‚Ä¢ pandavro: ‚≠ê‚≠ê‚≠ê (Good, but slower than fastavro)
    ‚Ä¢ avro-python3: ‚≠ê‚≠ê (Slowest, not recommended for large files)
    
    ORC:
    ‚Ä¢ PyArrow: ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê (Excellent performance, columnar optimizations)
    ‚Ä¢ pandas native: ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê (Same as PyArrow under the hood)
    
    üî• PYSPARK PERFORMANCE:
    ----------------------
    Avro:
    ‚Ä¢ Native PySpark: ‚≠ê‚≠ê‚≠ê‚≠ê (Good for distributed processing)
    ‚Ä¢ Best for: Large files, distributed computing
    
    ORC:
    ‚Ä¢ Native PySpark: ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê (Excellent, highly optimized)
    ‚Ä¢ Best for: Analytics workloads, data warehousing
    
    üìà RECOMMENDATIONS:
    ------------------
    Small-Medium files (<1GB):
    ‚Ä¢ Avro: Use pandas + fastavro
    ‚Ä¢ ORC: Use pandas + PyArrow
    
    Large files (>1GB):
    ‚Ä¢ Use PySpark for both formats
    ‚Ä¢ Leverage distributed processing
    
    Analytics workloads:
    ‚Ä¢ Prefer ORC format
    ‚Ä¢ Use PySpark for best performance
    
    Data exchange:
    ‚Ä¢ Prefer Avro format
    ‚Ä¢ Schema evolution capabilities
    """
    print(tips)

# ============================================================================
# INSTALLATION GUIDE
# ============================================================================

def installation_guide():
    """
    Complete installation guide
    """
    guide = """
    üì¶ INSTALLATION GUIDE
    ====================
    
    For Pandas + Avro:
    ------------------
    pip install pandas fastavro
    # OR
    pip install pandas pandavro
    
    For Pandas + ORC:
    ----------------
    pip install pandas pyarrow
    # Make sure pandas >= 1.5.0 for native ORC support
    
    For PySpark + Avro:
    ------------------
    pip install pyspark
    # Then configure Spark with spark-avro package:
    spark = SparkSession.builder \\
        .config("spark.jars.packages", "org.apache.spark:spark-avro_2.12:3.4.0") \\
        .getOrCreate()
    
    For PySpark + ORC:
    -----------------
    pip install pyspark
    # ORC support is built-in, no additional packages needed
    
    Complete setup for all formats:
    ------------------------------
    pip install pandas pyspark pyarrow fastavro pandavro
    """
    print(guide)

# Example usage
if __name__ == "__main__":
    installation_guide()
    print()
    performance_comparison_tips()
    print()
    
    # Uncomment to run comprehensive example:
    # comprehensive_example()
```
