# Spark Window Functions
Here is a PySpark Window functions cheatsheet with detailed examples and a reference table.I've created a comprehensive PySpark Window functions cheat sheet that includes:

**Key Sections:**
- **Basic Setup** - Imports and SparkSession creation
- **Window Specifications** - How to define window frames and partitions
- **Common Patterns** - Ranking, aggregates, and analytic functions with examples
- **Frame Specifications** - Different types of window frames (rows vs range)
- **Practical Examples** - Real-world scenarios like top N per group, running totals, moving averages
- **Reference Table** - Complete list of window functions organized by category
- **Performance Tips** - Best practices for optimization
- **Common Patterns** - Deduplication, time series analysis, and outlier detection

The reference table categorizes all window functions into three main types:
1. **Ranking Functions** - row_number(), rank(), dense_rank(), etc.
2. **Aggregate Functions** - sum(), avg(), count(), min(), max(), etc.  
3. **Analytic Functions** - lead(), lag(), first(), last(), nth_value()

- [Efficency Tips](#Efficency-Tips)
- [Spark SQL Windows Functions](#Spark-SQL-Windows-Functions)
- [Common Issues](#Common-Issues)

Each function includes practical examples showing the syntax and common use cases. The cheat sheet also covers important concepts like frame specifications (rows vs range) and performance optimization techniques.

---
# PySpark Window Functions Cheat Sheet

## Basic Setup and Imports

```python
from pyspark.sql import SparkSession
from pyspark.sql.window import Window
from pyspark.sql.functions import *

# Create SparkSession
spark = SparkSession.builder.appName("Window Functions").getOrCreate()
```

## Window Specification Basics

```python
# Basic window specification
window = Window.partitionBy("department").orderBy("salary")

# Window with frame specification
window_frame = Window.partitionBy("department") \
    .orderBy("salary") \
    .rowsBetween(Window.unboundedPreceding, Window.currentRow)

# Range-based window
range_window = Window.partitionBy("department") \
    .orderBy("salary") \
    .rangeBetween(-1000, 1000)
```

## Common Window Function Patterns

### Ranking Functions

```python
# Row number - unique sequential number
df.withColumn("row_num", row_number().over(window))

# Rank - same values get same rank, gaps in sequence
df.withColumn("rank", rank().over(window))

# Dense rank - same values get same rank, no gaps
df.withColumn("dense_rank", dense_rank().over(window))

# Percent rank - relative rank as percentage
df.withColumn("percent_rank", percent_rank().over(window))

# Ntile - divide into n buckets
df.withColumn("quartile", ntile(4).over(window))
```

### Aggregate Functions

```python
# Running sum
df.withColumn("running_sum", sum("salary").over(window_frame))

# Running average
df.withColumn("running_avg", avg("salary").over(window_frame))

# Running count
df.withColumn("running_count", count("*").over(window_frame))

# Running min/max
df.withColumn("running_min", min("salary").over(window_frame))
df.withColumn("running_max", max("salary").over(window_frame))
```

### Analytic Functions

```python
# Lead - value from next row
df.withColumn("next_salary", lead("salary", 1).over(window))

# Lag - value from previous row
df.withColumn("prev_salary", lag("salary", 1).over(window))

# First value in window
df.withColumn("first_salary", first("salary").over(window))

# Last value in window
df.withColumn("last_salary", last("salary").over(window))

# Nth value
df.withColumn("second_highest", nth_value("salary", 2).over(window))
```

## Frame Specifications

```python
# Unbounded frames
Window.unboundedPreceding  # Start of partition
Window.unboundedFollowing  # End of partition
Window.currentRow          # Current row

# Row-based frames (physical rows)
rows_window = Window.partitionBy("dept") \
    .orderBy("salary") \
    .rowsBetween(-2, 2)  # 2 rows before to 2 rows after

# Range-based frames (logical range based on order column)
range_window = Window.partitionBy("dept") \
    .orderBy("salary") \
    .rangeBetween(-1000, 1000)  # Salary within Â±1000
```

## Practical Examples

### Top N per Group

```python
# Get top 3 highest paid employees per department
top_n_window = Window.partitionBy("department").orderBy(desc("salary"))

result = df.withColumn("rank", row_number().over(top_n_window)) \
    .filter(col("rank") <= 3)
```

### Running Totals and Moving Averages

```python
# Running total
running_total_window = Window.partitionBy("account_id") \
    .orderBy("transaction_date") \
    .rowsBetween(Window.unboundedPreceding, Window.currentRow)

df.withColumn("running_balance", 
              sum("amount").over(running_total_window))

# 7-day moving average
moving_avg_window = Window.partitionBy("stock_symbol") \
    .orderBy("date") \
    .rowsBetween(-6, 0)

df.withColumn("moving_avg_7d", 
              avg("price").over(moving_avg_window))
```

### Percentage of Total

```python
# Calculate percentage of department total
dept_total_window = Window.partitionBy("department")

df.withColumn("dept_total", sum("salary").over(dept_total_window)) \
  .withColumn("pct_of_dept", (col("salary") / col("dept_total")) * 100)
```

### Gap Analysis

```python
# Find gaps between consecutive values
gap_window = Window.partitionBy("customer_id").orderBy("order_date")

df.withColumn("prev_order_date", lag("order_date").over(gap_window)) \
  .withColumn("days_since_last_order", 
              datediff("order_date", "prev_order_date"))
```

## Window Functions Reference Table

| Function Category | Function | Description | Example Usage |
|------------------|----------|-------------|---------------|
| **Ranking** | `row_number()` | Unique sequential number for each row | `row_number().over(window)` |
| | `rank()` | Rank with gaps for ties | `rank().over(window)` |
| | `dense_rank()` | Rank without gaps for ties | `dense_rank().over(window)` |
| | `percent_rank()` | Relative rank as percentage (0-1) | `percent_rank().over(window)` |
| | `ntile(n)` | Divide rows into n buckets | `ntile(4).over(window)` |
| **Aggregate** | `sum()` | Running/windowed sum | `sum("col").over(window)` |
| | `avg()` | Running/windowed average | `avg("col").over(window)` |
| | `count()` | Running/windowed count | `count("*").over(window)` |
| | `min()` | Running/windowed minimum | `min("col").over(window)` |
| | `max()` | Running/windowed maximum | `max("col").over(window)` |
| | `stddev()` | Standard deviation in window | `stddev("col").over(window)` |
| | `variance()` | Variance in window | `variance("col").over(window)` |
| **Analytic** | `lead(col, n)` | Value from n rows ahead | `lead("col", 1).over(window)` |
| | `lag(col, n)` | Value from n rows behind | `lag("col", 1).over(window)` |
| | `first(col)` | First value in window | `first("col").over(window)` |
| | `last(col)` | Last value in window | `last("col").over(window)` |
| | `nth_value(col, n)` | Nth value in window | `nth_value("col", 2).over(window)` |

## Frame Types Summary

| Frame Type | Syntax | Description |
|------------|--------|-------------|
| **Rows Frame** | `.rowsBetween(start, end)` | Physical number of rows |
| **Range Frame** | `.rangeBetween(start, end)` | Logical range based on ORDER BY column |
| **Unbounded** | `Window.unboundedPreceding` | From start of partition |
| | `Window.unboundedFollowing` | To end of partition |
| | `Window.currentRow` | Current row only |

## Performance Tips

```python
# Cache frequently used windows
common_window = Window.partitionBy("department").orderBy("salary")
# Reuse this window spec across multiple operations

# Avoid unnecessary sorting when possible
# Use partitionBy without orderBy for simple aggregations
simple_window = Window.partitionBy("department")

# Use appropriate frame bounds
# Default frame is RANGE UNBOUNDED PRECEDING AND CURRENT ROW
# Specify explicit bounds when different behavior is needed

# Consider using repartition() before window operations for better performance
df.repartition("department").withColumn("rank", rank().over(window))
```

## Common Patterns

### Deduplication

```python
# Remove duplicates keeping the latest record
dedup_window = Window.partitionBy("id").orderBy(desc("timestamp"))
df.withColumn("rn", row_number().over(dedup_window)) \
  .filter(col("rn") == 1) \
  .drop("rn")
```

### Time Series Analysis

```python
# Calculate period-over-period growth
growth_window = Window.partitionBy("product_id").orderBy("month")
df.withColumn("prev_sales", lag("sales").over(growth_window)) \
  .withColumn("growth_rate", 
              (col("sales") - col("prev_sales")) / col("prev_sales") * 100)
```

### Outlier Detection

```python
# Identify outliers using standard deviation
stats_window = Window.partitionBy("category")
df.withColumn("avg_price", avg("price").over(stats_window)) \
  .withColumn("stddev_price", stddev("price").over(stats_window)) \
  .withColumn("is_outlier", 
              abs(col("price") - col("avg_price")) > 2 * col("stddev_price"))
```

---
# Efficency Tips
Yes, it is **significantly more efficient** to apply multiple columns using the same window function in a single query. Here's why:

## Current Inefficient Approach
```python
# This creates multiple window operations
df_lag = df.withColumn("rolling_avg_temp", avg(col("readings.temperature")).over(time_window)) \
    .withColumn("lat_change", col("location.latitude") - lag("location.latitude").over(Window.partitionBy("sensor_id").orderBy("timestamp")))

df_timestats = df_lag.withColumn("rolling_avg_temp", avg(col("readings.temperature")).over(time_window)) \
    .withColumn("lat_change_hr", avg(col("lat_change")).over(time_window))
```

## Efficient Approach
```python
# Single operation with multiple window functions using same window spec
df_timestats = df_lag.withColumn("rolling_avg_temp", avg(col("readings.temperature")).over(time_window)) \
    .withColumn("lat_change_hr", avg(col("lat_change")).over(time_window))
```

## Why It's More Efficient: Backend Optimization

### 1. **Window Function Coalescing**
When Spark's Catalyst optimizer detects multiple window functions with the **same partitioning and ordering**, it performs "window function coalescing" - combining them into a single window operation.

**Without coalescing (inefficient):**
```
Window Operation 1: PARTITION BY sensor_id ORDER BY timestamp â rolling_avg_temp
Window Operation 2: PARTITION BY sensor_id ORDER BY timestamp â lat_change_hr
```

**With coalescing (efficient):**
```
Single Window Operation: PARTITION BY sensor_id ORDER BY timestamp â {rolling_avg_temp, lat_change_hr}
```

### 2. **Reduced Data Shuffling**
- **Partitioning**: Data needs to be shuffled/redistributed across nodes only **once** instead of multiple times
- **Sorting**: Within each partition, data is sorted only **once** instead of repeatedly
- **Network I/O**: Dramatically reduced network traffic between nodes

### 3. **Memory Efficiency**
- **Single Buffer**: Window frame data is loaded into memory once and reused for all calculations
- **Reduced Memory Pressure**: No need to materialize intermediate DataFrames between window operations
- **Cache Reuse**: The sorted, partitioned data stays in memory for all window functions

### 4. **CPU Efficiency**
- **Single Pass**: Data is processed in one pass instead of multiple passes
- **Reduced Overhead**: Less function call overhead and context switching

## Example: Physical Plan Comparison

**Inefficient (separate operations):**
```
== Physical Plan ==
Window [avg(temperature) ...]
+- Sort [sensor_id, timestamp]
   +- Exchange hashpartitioning(sensor_id)
      +- Window [avg(lat_change) ...]
         +- Sort [sensor_id, timestamp]  // Duplicate sort!
            +- Exchange hashpartitioning(sensor_id)  // Duplicate shuffle!
```

**Efficient (coalesced):**
```
== Physical Plan ==
Window [avg(temperature), avg(lat_change) ...]  // Single window with multiple functions
+- Sort [sensor_id, timestamp]  // Single sort
   +- Exchange hashpartitioning(sensor_id)  // Single shuffle
```

## Best Practice Code Structure

```python
from pyspark.sql.functions import avg, lag, col
from pyspark.sql.window import Window

# Define window specifications once
time_window = Window.partitionBy("sensor_id").orderBy("timestamp").rowsBetween(-11, 0)
lag_window = Window.partitionBy("sensor_id").orderBy("timestamp")

# Step 1: Calculate lag-based columns
df_with_lag = df.withColumn("lat_change", 
    col("location.latitude") - lag("location.latitude").over(lag_window))

# Step 2: Apply all rolling window functions at once (EFFICIENT!)
df_final = df_with_lag \
    .withColumn("rolling_avg_temp", avg(col("readings.temperature")).over(time_window)) \
    .withColumn("rolling_avg_lat_change", avg(col("lat_change")).over(time_window)) \
    .withColumn("rolling_max_temp", max(col("readings.temperature")).over(time_window)) \
    .withColumn("rolling_sum_lat_change", sum(col("lat_change")).over(time_window))
```

## Performance Impact
In large datasets, this optimization can provide:
- **2-5x faster execution** for window-heavy workloads
- **50-80% reduction** in network shuffle
- **30-60% less memory usage**
- **Better parallelization** across cluster nodes

The key is that Spark's Catalyst optimizer is very good at detecting and optimizing identical window specifications, but it can only do this within the same transformation stage.
---
# Spark SQL Windows Functions
Here are simple Spark SQL window function examples with explanations:

## Basic Setup Data
```sql
-- Sample sales data
CREATE OR REPLACE TEMPORARY VIEW sales AS
SELECT * FROM VALUES
  ('Alice', 'Q1', 1000),
  ('Alice', 'Q2', 1200),
  ('Alice', 'Q3', 900),
  ('Alice', 'Q4', 1500),
  ('Bob', 'Q1', 800),
  ('Bob', 'Q2', 1100),
  ('Bob', 'Q3', 1300),
  ('Bob', 'Q4', 1000)
AS sales(salesperson, quarter, amount);
```

## 1. ROW_NUMBER() - Ranking within partitions
```sql
SELECT 
    salesperson,
    quarter,
    amount,
    ROW_NUMBER() OVER (PARTITION BY salesperson ORDER BY amount DESC) as rank_by_amount
FROM sales;

-- Output:
-- Alice  Q4  1500  1
-- Alice  Q2  1200  2  
-- Alice  Q1  1000  3
-- Alice  Q3   900  4
-- Bob    Q3  1300  1
-- Bob    Q2  1100  2
-- Bob    Q4  1000  3
-- Bob    Q1   800  4
```

## 2. SUM() with ROWS BETWEEN - Running totals
```sql
SELECT 
    salesperson,
    quarter,
    amount,
    SUM(amount) OVER (
        PARTITION BY salesperson 
        ORDER BY quarter 
        ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW
    ) as running_total
FROM sales;

-- Output:
-- Alice  Q1  1000  1000  (1000)
-- Alice  Q2  1200  2200  (1000+1200)  
-- Alice  Q3   900  3100  (1000+1200+900)
-- Alice  Q4  1500  4600  (1000+1200+900+1500)
-- Bob    Q1   800   800
-- Bob    Q2  1100  1900
-- Bob    Q3  1300  3200
-- Bob    Q4  1000  4200
```

## 3. AVG() with ROWS BETWEEN - Rolling average (3-period)
```sql
SELECT 
    salesperson,
    quarter,
    amount,
    AVG(amount) OVER (
        PARTITION BY salesperson 
        ORDER BY quarter 
        ROWS BETWEEN 2 PRECEDING AND CURRENT ROW
    ) as rolling_avg_3
FROM sales;

-- Output:
-- Alice  Q1  1000  1000.0     (just Q1)
-- Alice  Q2  1200  1100.0     (Q1+Q2)/2
-- Alice  Q3   900  1033.33    (Q1+Q2+Q3)/3
-- Alice  Q4  1500  1200.0     (Q2+Q3+Q4)/3  <-- rolling window
-- Bob    Q1   800   800.0
-- Bob    Q2  1100   950.0
-- Bob    Q3  1300  1066.67
-- Bob    Q4  1000  1133.33
```

## 4. LAG() and LEAD() - Previous/Next values
```sql
SELECT 
    salesperson,
    quarter,
    amount,
    LAG(amount, 1) OVER (PARTITION BY salesperson ORDER BY quarter) as prev_quarter,
    LEAD(amount, 1) OVER (PARTITION BY salesperson ORDER BY quarter) as next_quarter,
    amount - LAG(amount, 1) OVER (PARTITION BY salesperson ORDER BY quarter) as change_from_prev
FROM sales;

-- Output:
-- Alice  Q1  1000  NULL   1200   NULL
-- Alice  Q2  1200  1000    900    200
-- Alice  Q3   900  1200   1500   -300
-- Alice  Q4  1500   900   NULL    600
-- Bob    Q1   800  NULL   1100   NULL
-- Bob    Q2  1100   800   1300    300
-- Bob    Q3  1300  1100   1000    200
-- Bob    Q4  1000  1300   NULL   -300
```

## 5. ROWS BETWEEN Examples - Different frame specifications

### 5a. Fixed window size (last 2 rows + current)
```sql
SELECT 
    salesperson,
    quarter,
    amount,
    MAX(amount) OVER (
        PARTITION BY salesperson 
        ORDER BY quarter 
        ROWS BETWEEN 2 PRECEDING AND CURRENT ROW
    ) as max_last_3_quarters
FROM sales;
```

### 5b. Centered window (1 before + current + 1 after)
```sql
SELECT 
    salesperson,
    quarter,
    amount,
    AVG(amount) OVER (
        PARTITION BY salesperson 
        ORDER BY quarter 
        ROWS BETWEEN 1 PRECEDING AND 1 FOLLOWING
    ) as centered_avg
FROM sales;
```

### 5c. Future-looking window (current + next 2)
```sql
SELECT 
    salesperson,
    quarter,
    amount,
    MIN(amount) OVER (
        PARTITION BY salesperson 
        ORDER BY quarter 
        ROWS BETWEEN CURRENT ROW AND 2 FOLLOWING
    ) as min_next_3_quarters
FROM sales;
```

## 6. RANGE BETWEEN - Value-based windows
```sql
-- For time-series data with dates
CREATE OR REPLACE TEMPORARY VIEW daily_sales AS
SELECT * FROM VALUES
  ('2024-01-01', 100),
  ('2024-01-02', 150),
  ('2024-01-03', 120),
  ('2024-01-04', 180),
  ('2024-01-05', 200)
AS daily_sales(date, amount);

SELECT 
    date,
    amount,
    SUM(amount) OVER (
        ORDER BY date 
        RANGE BETWEEN INTERVAL 2 DAYS PRECEDING AND CURRENT ROW
    ) as sum_last_3_days
FROM daily_sales;
```

## 7. Complex Example - Multiple window functions with WINDOW clause
```sql
SELECT 
    salesperson,
    quarter,
    amount,
    -- Ranking functions
    ROW_NUMBER() OVER w_amount as row_num,
    RANK() OVER w_amount as rank_amount,
    DENSE_RANK() OVER w_amount as dense_rank,
    
    -- Aggregate functions with frames
    SUM(amount) OVER w_running as running_total,
    AVG(amount) OVER w_rolling as rolling_avg,
    MAX(amount) OVER w_rolling as rolling_max,
    
    -- Offset functions
    LAG(amount, 1) OVER w_time as prev_amount,
    LEAD(amount, 1) OVER w_time as next_amount
    
FROM sales
WINDOW 
    w_amount AS (PARTITION BY salesperson ORDER BY amount DESC),
    w_running AS (PARTITION BY salesperson ORDER BY quarter ROWS UNBOUNDED PRECEDING),
    w_rolling AS (PARTITION BY salesperson ORDER BY quarter ROWS BETWEEN 1 PRECEDING AND 1 FOLLOWING),
    w_time AS (PARTITION BY salesperson ORDER BY quarter);
```

## Key ROWS BETWEEN Syntax Options:
- `ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW` - From start to current
- `ROWS BETWEEN 2 PRECEDING AND CURRENT ROW` - Last 3 rows (including current)
- `ROWS BETWEEN 1 PRECEDING AND 1 FOLLOWING` - 3-row centered window
- `ROWS BETWEEN CURRENT ROW AND 2 FOLLOWING` - Current + next 2 rows
- `ROWS BETWEEN CURRENT ROW AND UNBOUNDED FOLLOWING` - Current to end

The `ROWS BETWEEN` clause is essential for creating rolling calculations, moving averages, and other sliding window analytics in Spark SQL.


You **can** use multiple window functions in Spark SQL queries, but there are important rules and best practices to follow:

## What's Allowed

**1. Multiple window functions in the same SELECT statement:**
```sql
SELECT 
    sensor_id,
    timestamp,
    temperature,
    -- Multiple window functions are fine
    AVG(temperature) OVER (PARTITION BY sensor_id ORDER BY timestamp ROWS BETWEEN 5 PRECEDING AND CURRENT ROW) as rolling_avg,
    LAG(temperature, 1) OVER (PARTITION BY sensor_id ORDER BY timestamp) as prev_temp,
    ROW_NUMBER() OVER (PARTITION BY sensor_id ORDER BY timestamp DESC) as rn
FROM sensor_data
```

**2. Different window specifications:**
```sql
SELECT 
    sensor_id,
    temperature,
    AVG(temperature) OVER (PARTITION BY sensor_id ORDER BY timestamp) as cumulative_avg,
    MAX(temperature) OVER (PARTITION BY location ORDER BY timestamp) as location_max
FROM sensor_data
```

## What's NOT Allowed

**You cannot use the result of one window function as input to another window function in the same query level:**

```sql
-- This will fail
SELECT 
    sensor_id,
    temperature,
    LAG(temperature) OVER (PARTITION BY sensor_id ORDER BY timestamp) as prev_temp,
    AVG(prev_temp) OVER (PARTITION BY sensor_id ORDER BY timestamp) as avg_of_prev  -- ERROR!
FROM sensor_data
```

## Solutions for Nested Window Operations

**1. Use CTEs (Common Table Expressions):**
```sql
WITH step1 AS (
    SELECT 
        sensor_id,
        timestamp,
        temperature,
        LAG(temperature, 1) OVER (PARTITION BY sensor_id ORDER BY timestamp) as prev_temp
    FROM sensor_data
)
SELECT 
    sensor_id,
    timestamp,
    temperature,
    prev_temp,
    AVG(prev_temp) OVER (PARTITION BY sensor_id ORDER BY timestamp) as rolling_avg_prev
FROM step1
```

**2. Use subqueries:**
```sql
SELECT 
    sensor_id,
    timestamp,
    temperature,
    prev_temp,
    AVG(prev_temp) OVER (PARTITION BY sensor_id ORDER BY timestamp) as rolling_avg_prev
FROM (
    SELECT 
        sensor_id,
        timestamp,
        temperature,
        LAG(temperature, 1) OVER (PARTITION BY sensor_id ORDER BY timestamp) as prev_temp
    FROM sensor_data
) t
```

## Performance Considerations

**1. Window function optimization:**
Spark can optimize multiple window functions that use the same partitioning and ordering:
```sql
-- These will be optimized together
SELECT 
    sensor_id,
    AVG(temperature) OVER w as avg_temp,
    MAX(temperature) OVER w as max_temp,
    MIN(temperature) OVER w as min_temp
FROM sensor_data
WINDOW w AS (PARTITION BY sensor_id ORDER BY timestamp ROWS BETWEEN 10 PRECEDING AND CURRENT ROW)
```

**2. Use WINDOW clause for reusability:**
```sql
SELECT 
    sensor_id,
    timestamp,
    temperature,
    AVG(temperature) OVER sensor_window as rolling_avg,
    LAG(temperature, 1) OVER time_window as prev_temp,
    LEAD(temperature, 1) OVER time_window as next_temp
FROM sensor_data
WINDOW 
    sensor_window AS (PARTITION BY sensor_id ORDER BY timestamp ROWS BETWEEN 5 PRECEDING AND CURRENT ROW),
    time_window AS (PARTITION BY sensor_id ORDER BY timestamp)
```

## Summary

- **Multiple window functions in one query**: â Allowed
- **Different window specifications**: â Allowed  
- **Nested window functions**: â Not allowed directly
- **Solution**: Use CTEs or subqueries to break into multiple steps
- **Optimization**: Use WINDOW clause and similar partitioning for better performance

The key is that each window function operates on the base table columns or computed non-window columns, not on the results of other window functions in the same query level.

---
# Common Issues
## Using Multiple Windows Functions in Same withColumn Chain
When working with a set of windows functions and you want to chain a column created by a windows function into another windows function (Common in time-series data), you cannot create both columns in the same withColumn chain. This is different from stacking column hygiene operations for pyspark withColumn chains
**Example Hygiene withColumn Stack**
```python
df_coast = df.withColumn("latitude_block", round(col("location.latitude"), 4)) \
    .withColumn("region", when(col("latitude_block").between(40.7, 40.8), "East Coast") \
        .when(col("latitude_block").between(40.6, 40.7), "Central") \
        .when(col("latitude_block").between(40.5, 40.6), "West Coast") \
        .otherwise("Out-of-Bounds")) 

df_coast.show(truncate=False)
```

**Bad Code:**
```python
from pyspark.sql.functions import *
from pyspark.sql.window import Window

time_window = Window.partitionBy("sensor_id").orderBy("timestamp").rowsBetween(-11,0) # a rolling hours

df_timestats = df.withColumn("rolliing_avg_temp", avg(col("readings.temperature")).over(time_window)) \
    .withColumn("lat_change", col("location.latitude") - lag("location.latitude").over(Window.partitionBy("sensor_id").orderBy("timestamp"))) \
    .withColumn("lat_change_hr", col("lat_change").over(time_window))
```
**Error (First Line is Most Important**
```
AnalysisException: [UNSUPPORTED_EXPR_FOR_WINDOW] Expression "lat_change" not supported within a window function.;
Project [sensor_id#31, timestamp#32, location#33, readings#34, status#35, minute_15_bucket#211, rolliing_avg_temp#285, lat_change#295, lat_change_hr#308]
+- Project [sensor_id#31, timestamp#32, location#33, readings#34, status#35, minute_15_bucket#211, rolliing_avg_temp#285, lat_change#295, lat_change_hr#308, lat_change_hr#308]
   +- Window [lat_change#295 windowspecdefinition(sensor_id#31, timestamp#32 ASC NULLS FIRST, specifiedwindowframe(RowFrame, -11, currentrow$())) AS lat_change_hr#308], [sensor_id#31], [timestamp#32 ASC NULLS FIRST]
      +- Project [sensor_id#31, timestamp#32, location#33, readings#34, status#35, minute_15_bucket#211, rolliing_avg_temp#285, lat_change#295]
         +- Project [sensor_id#31, timestamp#32, location#33, readings#34, status#35, minute_15_bucket#211, rolliing_avg_temp#285, lat_change#295]
            +- Project [sensor_id#31, timestamp#32, location#33, readings#34, status#35, minute_15_bucket#211, rolliing_avg_temp#285, _w0#298, _we0#299, (location#33.latitude - _we0#299) AS lat_change#295]
               +- Window [lag(_w0#298, -1, null) windowspecdefinition(sensor_id#31, timestamp#32 ASC NULLS FIRST, specifiedwindowframe(RowFrame, -1, -1)) AS _we0#299], [sensor_id#31], [timestamp#32 ASC NULLS FIRST]
                  +- Project [sensor_id#31, timestamp#32, location#33, readings#34, status#35, minute_15_bucket#211, rolliing_avg_temp#285, location#33.latitude AS _w0#298]
                     +- Project [sensor_id#31, timestamp#32, location#33, readings#34, status#35, minute_15_bucket#211, rolliing_avg_temp#285]
                        +- Project [sensor_id#31, timestamp#32, location#33, readings#34, status#35, minute_15_bucket#211, _w0#287, rolliing_avg_temp#285, rolliing_avg_temp#285]
                           +- Window [avg(_w0#287) windowspecdefinition(sensor_id#31, timestamp#32 ASC NULLS FIRST, specifiedwindowframe(RowFrame, -11, currentrow$())) AS rolliing_avg_temp#285], [sensor_id#31], [timestamp#32 ASC NULLS FIRST]
                              +- Project [sensor_id#31, timestamp#32, location#33, readings#34, status#35, minute_15_bucket#211, readings#34[temperature] AS _w0#287]
                                 +- Project [sensor_id#31, timestamp#32, location#33, readings#34, status#35, cast(from_unixtime((FLOOR((cast(unix_timestamp(timestamp#32, yyyy-MM-dd HH:mm:ss, Some(America/New_York), false) as double) / cast(900 as double))) * cast(900 as bigint)), yyyy-MM-dd HH:mm:ss, Some(America/New_York)) as timestamp) AS minute_15_bucket#211]
                                    +- LogicalRDD [sensor_id#31, timestamp#32, location#33, readings#34, status#35], false
```
